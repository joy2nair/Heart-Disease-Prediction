# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1blPveZgsGcxhmMlCX22rHWPnSamH3QN5
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# Loading the Heart dataset
heart_df = pd.read_csv("/content/drive/MyDrive/heart.csv")
heart_df

heart_df.sample(5)

heart_df.info()

heart_df.describe()

heart_df.describe(include="all")

"""## Data Preprocessing"""

heart_df.isna().sum()

heart_df.duplicated().sum()

heart_df.nunique()

cat_col = heart_df.select_dtypes(include='object').columns

heart_df['ChestPainType'].unique()

"""## Converting Categorical Variable To Numerics

*   Sex : M = 0, F = 1
*   ChestPainType : ATA = 0, NAP = 1,ASY = 2,TA = 3
*   RestingECG : Normal = 0, ST = 1, LVH = 2
*   ExerciseAngina : N = 0, Y = 1
*   ST_Slope : Up = 0, Flat = 1, Down = 2






"""

range(heart_df['ChestPainType'].nunique())

for col in cat_col:
  print((heart_df[col].unique(),list(range(heart_df[col].nunique()))))
  heart_df[col].replace((heart_df[col].unique()), range(heart_df[col].nunique()), inplace=True)
  print('*'*90)
  print()

heart_df

heart_df['Cholesterol'].value_counts()

"""Cholesterol cannot be 0,it states that the reading was not properly noted.

# Imputing the 0 values in chlesterol column with KNN Imputer
"""

np.nan

heart_df['Cholesterol'].replace(0, np.nan, inplace=True)

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors = 3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd.DataFrame(after_impute, columns=heart_df.columns)

heart_df

count = 0
for i in heart_df['Cholesterol']:
  if i == 0:
    count += 1
    print(count)

"""## Doing the same for Resting Blood Pressure"""

heart_df['RestingBP'][heart_df['RestingBP'] == 0]

from sklearn.impute import KNNImputer
heart_df['RestingBP'].replace(0, np.nan, inplace=True)
imputer = KNNImputer(n_neighbors = 3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd. DataFrame(after_impute, columns=heart_df.columns)

heart_df['RestingBP'].unique()

heart_df['RestingBP'].isnull().sum()

"""## Change columns type to int"""

withoutOldPeak = heart_df.columns
withoutOldPeak = withoutOldPeak.drop('Oldpeak')
heart_df[withoutOldPeak] = heart_df[withoutOldPeak].astype('int32')

heart_df.info()

heart_df.sample()

heart_df.corr()['HeartDisease'][:-1].sort_values()

"""## Data Visualization"""

import plotly.express as px

px.line(heart_df.corr()['HeartDisease'][:-1].sort_values())

"""## Age and HeartDisease Distribution"""

px.sunburst(heart_df,path = ['HeartDisease','Age'])

px.histogram(heart_df,x='Age',color='HeartDisease')

"""## Percentage of HeartDisease data distribution"""

px.pie(heart_df,names='HeartDisease',title='Percentage of HeartDisease classes distribution')

"""## Sex vs Heart DIsease"""

px.histogram(heart_df,x='Sex',color='HeartDisease')

"""##CheastPainType vs HeartDisease"""

px.histogram(heart_df,x='ChestPainType',color='HeartDisease')
# ChestPainType : ATA = 0 , NAP = 1, ASY = 2, TA = 3

"""## RestingBP vs HeartDisease"""

heart_df['RestingBP'].unique()

px.sunburst(heart_df,path = ['HeartDisease','RestingBP'])

"""## FastingBS vs Heart DIsease"""

px.histogram(heart_df,x='FastingBS',color='HeartDisease')

"""## MaxHR vs Heart Disease"""

px.sunburst(heart_df,path = ['HeartDisease','MaxHR'])

px.violin(heart_df,x='HeartDisease',y='MaxHR',color='HeartDisease')

"""## Oldpeak vs Heart Disease"""

px.violin(heart_df,x='HeartDisease',y='Oldpeak',color='HeartDisease')

"""## ST_Slope vs Heart Disease"""

px.histogram(heart_df,x='ST_Slope',color='HeartDisease')

"""## Exercise Angina vs Heart Disease"""

px.histogram(heart_df,x='ExerciseAngina',color='HeartDisease')

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    heart_df.drop('HeartDisease', axis = 1),
    heart_df['HeartDisease'],
    test_size=0.2,
    random_state = 42,
    stratify = heart_df['HeartDisease']
)









"""## Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score


solver = ['lbfgs', 'liblinear', 'newton-cg','newton-cholesky','sag', 'saga']
best_solver = ''
test_score = np.zeros(6)
for i, n in enumerate(solver):
  lr = LogisticRegression(solver = n).fit(X_train, y_train)
  test_score[i] = lr.score(X_test, y_test)
  if lr.score(X_test, y_test) == test_score.max():
    best_solver = n

print(best_solver)
lr = LogisticRegression(solver=best_solver)
lr.fit(X_train, y_train)
lr_pred = lr.predict(X_test)
print(f'LogisticRegression Score: {accuracy_score(y_test,lr_pred)}')

"""## Support Vector Machine (SVM)"""

from sklearn.svm import SVC
from sklearn.metrics import f1_score

kernels = {'linear':0, 'poly':0, 'rbf':0, 'sigmoid':0}
best = ''
for i in kernels:
  svm = SVC(kernel=i)
  svm.fit(X_train, y_train)
  yhat = svm.predict(X_test)
  kernels[i] =f1_score(y_test,yhat, average="weighted")
  if kernels[i] == max(kernels.values()):
    best = i

svm =SVC(kernel=best)
svm.fit(X_train, y_train)
svm_pred = svm.predict(X_test)
print(f'svm f1+score kernel({best}): {f1_score(y_test,svm_pred, average="weighted")}')

"""## Decision Tree Classifier"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

dtree = DecisionTreeClassifier(class_weight='balanced')
param_grid = {
    'max_depth': [3,4,5,6,7,8],
    'min_samples_split': [2,3,4],
    'min_samples_leaf':[1,2,3,4],
    'random_state' : [0, 42],
}
grid_search = GridSearchCV(dtree, param_grid,cv=5)
grid_search.fit(X_train, y_train)
Ctree = DecisionTreeClassifier(**grid_search.best_params_, class_weight='balanced')
Ctree.fit(X_train, y_train)
dtc_pred = Ctree.predict(X_test)
print("DecisionTrees's Accuracy:", accuracy_score(y_test, dtc_pred))

"""## Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rfc = RandomForestClassifier()
param_grid = {
    'n_estimators': [50, 100, 150, 500],
    'max_features' : ['sqrt', 'log2', None],
    'max_depth': [3, 6, 9, 19],
    'max_leaf_nodes': [3, 6, 9],
}
grid_search = GridSearchCV(rfc, param_grid)
grid_search.fit(X_train, y_train)
rfctree = RandomForestClassifier(**grid_search.best_params_)
rfctree.fit(X_train, y_train)
rfc_pred = rfctree.predict(X_test)
print("DecisionTrees's Accuracy:", accuracy_score(y_test, rfc_pred))